{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning SentenceTransformers for Multilingual Entity Resolution\n",
    "\n",
    "This notebook demonstrates how to fine-tune a SentenceTransformer model for entity matching across languages and character sets. We use contrastive learning to train a model that can compare people and company names, measuring similarity even when the names are in different languages or scripts.\n",
    "\n",
    "## Background\n",
    "\n",
    "Entity resolution for person and company names is challenging because:\n",
    "1. Names vary across cultures and languages\n",
    "2. Transliteration creates variations (e.g., \"Yevgeny Prigozhin\" vs \"Евгений Пригожин\")\n",
    "3. Abbreviations and different forms (e.g., \"John Smith\" vs \"J. Smith\")\n",
    "\n",
    "Traditional string matching methods often fail with these variations. We use representation learning to create embeddings that capture semantic meaning across languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We start by importing the required libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.quantization as tq\n",
    "from datasets import Dataset  # type: ignore\n",
    "from scipy.stats import iqr\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    losses,\n",
    ")\n",
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "from sentence_transformers.model_card import SentenceTransformerModelCardData\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.integrations import WandbCallback\n",
    "\n",
    "import wandb\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_SEED = 31337\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.mps.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Setup logging and suppress warnings\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# HuggingFace settings\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co/\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Configure Pandas to show more rows\n",
    "pd.set_option(\"display.max_rows\", 40)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "First, let's define some utility functions to help with our training and evaluation. These are imported from `eridu.train.utils` in the full codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_sbert_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy, precision, recall, f1 and roc_auc\n",
    "    \n",
    "    This function is called during model evaluation and logs metrics to W&B automatically\n",
    "    through the WandbCallback.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Apply threshold to predictions (0.5 is default)\n",
    "    if isinstance(predictions[0], float):\n",
    "        # If predictions are similarity scores (between 0 and 1)\n",
    "        binary_preds = [1 if pred >= 0.5 else 0 for pred in predictions]\n",
    "    else:\n",
    "        # If predictions are already binary\n",
    "        binary_preds = predictions\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, binary_preds),\n",
    "        \"precision\": precision_score(labels, binary_preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, binary_preds, zero_division=0),\n",
    "        \"f1\": f1_score(labels, binary_preds, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    # Calculate AUC only if predictions are continuous (not binary)\n",
    "    if isinstance(predictions[0], float):\n",
    "        metrics[\"auc\"] = roc_auc_score(labels, predictions)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def sbert_compare(sbert_model, name1, name2, use_gpu=True):\n",
    "    \"\"\"Compare two names using SBERT embeddings and cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        sbert_model: The SentenceTransformer model to use\n",
    "        name1: First name to compare\n",
    "        name2: Second name to compare\n",
    "        use_gpu: Whether to use GPU acceleration\n",
    "        \n",
    "    Returns:\n",
    "        Cosine similarity between the two name embeddings (0-1)\n",
    "    \"\"\"\n",
    "    # Get the device from the model\n",
    "    device = next(sbert_model.parameters()).device if use_gpu else torch.device(\"cpu\")\n",
    "    device_str = str(device)\n",
    "    \n",
    "    # Determine whether to use GPU based on availability and parameter\n",
    "    convert_to_tensor = use_gpu and (device.type == \"cuda\" or device.type == \"mps\")\n",
    "    \n",
    "    # Encode both names into embeddings\n",
    "    if convert_to_tensor:\n",
    "        # GPU path\n",
    "        embedding1_tensor = sbert_model.encode(\n",
    "            name1,\n",
    "            convert_to_tensor=True,\n",
    "            convert_to_numpy=False,\n",
    "            device=device_str,\n",
    "        )\n",
    "        embedding2_tensor = sbert_model.encode(\n",
    "            name2,\n",
    "            convert_to_tensor=True,\n",
    "            convert_to_numpy=False,\n",
    "            device=device_str,\n",
    "        )\n",
    "        \n",
    "        # Handle dimensions\n",
    "        if len(embedding1_tensor.shape) > 1 and embedding1_tensor.shape[0] == 1:\n",
    "            embedding1_tensor = embedding1_tensor.squeeze(0)\n",
    "            embedding2_tensor = embedding2_tensor.squeeze(0)\n",
    "            \n",
    "        # Normalize the embeddings\n",
    "        embedding1_tensor = embedding1_tensor / torch.norm(embedding1_tensor)\n",
    "        embedding2_tensor = embedding2_tensor / torch.norm(embedding2_tensor)\n",
    "        \n",
    "        # For a single vector, use a simple dot product\n",
    "        similarity = torch.sum(embedding1_tensor * embedding2_tensor).item()\n",
    "        return float(similarity)\n",
    "    else:\n",
    "        # CPU implementation\n",
    "        from scipy.spatial import distance\n",
    "        embedding1_np = sbert_model.encode(name1, convert_to_numpy=True)\n",
    "        embedding2_np = sbert_model.encode(name2, convert_to_numpy=True)\n",
    "        diff = 1 - distance.cosine(embedding1_np, embedding2_np)\n",
    "        return diff\n",
    "\n",
    "def sbert_compare_multiple(sbert_model, names1, names2, use_gpu=True):\n",
    "    \"\"\"Compare multiple pairs of names efficiently.\n",
    "    \n",
    "    Args:\n",
    "        sbert_model: The SentenceTransformer model to use\n",
    "        names1: List of first names to compare\n",
    "        names2: List of second names to compare\n",
    "        use_gpu: Whether to use GPU acceleration\n",
    "        \n",
    "    Returns:\n",
    "        Array of cosine similarities between corresponding name pairs\n",
    "    \"\"\"\n",
    "    # Handle pandas Series\n",
    "    if isinstance(names1, pd.Series):\n",
    "        names1 = names1.astype(str).tolist()\n",
    "    if isinstance(names2, pd.Series):\n",
    "        names2 = names2.astype(str).tolist()\n",
    "    \n",
    "    # Get device\n",
    "    device = next(sbert_model.parameters()).device if use_gpu else torch.device(\"cpu\")\n",
    "    device_str = str(device)\n",
    "    \n",
    "    # Determine whether to use GPU\n",
    "    convert_to_tensor = use_gpu and (device.type == \"cuda\" or device.type == \"mps\")\n",
    "    \n",
    "    if convert_to_tensor:\n",
    "        # GPU path\n",
    "        embeddings1_tensor = sbert_model.encode(\n",
    "            names1,\n",
    "            convert_to_tensor=True,\n",
    "            convert_to_numpy=False,\n",
    "            device=device_str,\n",
    "        )\n",
    "        embeddings2_tensor = sbert_model.encode(\n",
    "            names2,\n",
    "            convert_to_tensor=True,\n",
    "            convert_to_numpy=False,\n",
    "            device=device_str,\n",
    "        )\n",
    "        \n",
    "        # Handle small sample sizes\n",
    "        if len(embeddings1_tensor.shape) == 1:\n",
    "            embeddings1_tensor = embeddings1_tensor.unsqueeze(0)\n",
    "            embeddings2_tensor = embeddings2_tensor.unsqueeze(0)\n",
    "            \n",
    "        # Normalize along embedding dimension\n",
    "        embeddings1_tensor = embeddings1_tensor / torch.norm(embeddings1_tensor, dim=1, keepdim=True)\n",
    "        embeddings2_tensor = embeddings2_tensor / torch.norm(embeddings2_tensor, dim=1, keepdim=True)\n",
    "        \n",
    "        # Compute similarity\n",
    "        tensor_similarities = torch.sum(embeddings1_tensor * embeddings2_tensor, dim=1)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        similarities = tensor_similarities.cpu().numpy()\n",
    "    else:\n",
    "        # CPU path\n",
    "        embeddings1_np = sbert_model.encode(names1, convert_to_numpy=True)\n",
    "        embeddings2_np = sbert_model.encode(names2, convert_to_numpy=True)\n",
    "        \n",
    "        # Handle small sample sizes\n",
    "        if len(embeddings1_np.shape) == 1:\n",
    "            embeddings1_np = np.expand_dims(embeddings1_np, axis=0)\n",
    "            embeddings2_np = np.expand_dims(embeddings2_np, axis=0)\n",
    "            \n",
    "        # Normalize\n",
    "        embeddings1_np = embeddings1_np / np.linalg.norm(embeddings1_np, axis=1, keepdims=True)\n",
    "        embeddings2_np = embeddings2_np / np.linalg.norm(embeddings2_np, axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarities = np.sum(embeddings1_np * embeddings2_np, axis=1)\n",
    "        \n",
    "    return similarities\n",
    "\n",
    "def sbert_compare_multiple_df(sbert_model, names1, names2, matches, use_gpu=True):\n",
    "    \"\"\"Compute similarities and return as DataFrame.\"\"\"\n",
    "    similarities = sbert_compare_multiple(sbert_model, names1, names2, use_gpu=use_gpu)\n",
    "    return pd.DataFrame(\n",
    "        {\"name1\": names1, \"name2\": names2, \"similarity\": similarities, \"match\": matches}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Now, let's set up the configuration parameters for our training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure sample size and model training parameters\n",
    "SAMPLE_FRACTION = 0.01  # Fraction of data to use (set lower for faster testing)\n",
    "SBERT_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "VARIANT = \"original\"\n",
    "OPTIMIZER = \"adafactor\"\n",
    "MODEL_SAVE_NAME = (SBERT_MODEL + \"-\" + VARIANT + \"-\" + OPTIMIZER).replace(\"/\", \"-\")\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 1024\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "PATIENCE = 2\n",
    "LEARNING_RATE = 5e-5\n",
    "SBERT_OUTPUT_FOLDER = f\"data/fine-tuned-sbert-{MODEL_SAVE_NAME}\"\n",
    "SAVE_EVAL_STEPS = 100\n",
    "USE_FP16 = True\n",
    "\n",
    "# Weights & Biases configuration\n",
    "WANDB_PROJECT = \"eridu\"\n",
    "WANDB_ENTITY = \"your_wandb_username\"  # Replace with your W&B username\n",
    "\n",
    "# GPU configuration\n",
    "USE_GPU = True\n",
    "\n",
    "# Display the configuration\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Model: {SBERT_MODEL}\")\n",
    "print(f\"  Sample fraction: {SAMPLE_FRACTION}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  FP16: {USE_FP16}\")\n",
    "print(f\"  GPU enabled: {USE_GPU}\")\n",
    "print(f\"  Output folder: {SBERT_OUTPUT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights & Biases\n",
    "\n",
    "We'll use Weights & Biases for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Weights & Biases\n",
    "# You need to run wandb.login() first if you haven't already\n",
    "wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    config={\n",
    "        \"variant\": VARIANT,\n",
    "        \"optimizer\": OPTIMIZER,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"sample_fraction\": SAMPLE_FRACTION,\n",
    "        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"use_fp16\": USE_FP16,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"sbert_model\": SBERT_MODEL,\n",
    "        \"model_save_name\": MODEL_SAVE_NAME,\n",
    "        \"sbert_output_folder\": SBERT_OUTPUT_FOLDER,\n",
    "        \"save_eval_steps\": SAVE_EVAL_STEPS,\n",
    "    },\n",
    "    save_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setup\n",
    "\n",
    "Check for GPU availability and set the device accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for CUDA or MPS availability and set the device\n",
    "if USE_GPU:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple GPU (Metal) acceleration\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using NVIDIA CUDA GPU acceleration\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"No GPU available, falling back to CPU\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"GPU disabled, using CPU for training\")\n",
    "\n",
    "print(f\"Device for fine-tuning SBERT: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Load the labeled pairs dataset and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_parquet(\"data/pairs-all.parquet\")\n",
    "\n",
    "# Display a sample of the raw data\n",
    "print(\"\\nRaw training data sample:\\n\")\n",
    "display(dataset.sample(n=5))\n",
    "\n",
    "# Sample the dataset if needed\n",
    "if SAMPLE_FRACTION < 1.0:\n",
    "    dataset = dataset.sample(frac=SAMPLE_FRACTION, random_state=RANDOM_SEED)\n",
    "\n",
    "# Split into train, eval, and test sets\n",
    "train_df, tmp_df = train_test_split(dataset, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "eval_df, test_df = train_test_split(tmp_df, test_size=0.5, random_state=RANDOM_SEED, shuffle=True)\n",
    "\n",
    "print(f\"\\nTraining data:   {len(train_df):,}\")\n",
    "print(f\"Evaluation data: {len(eval_df):,}\")\n",
    "print(f\"Test data:       {len(test_df):,}\\n\")\n",
    "\n",
    "# Convert to HuggingFace Datasets format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": train_df[\"left_name\"].tolist(),\n",
    "    \"sentence2\": train_df[\"right_name\"].tolist(),\n",
    "    \"label\": train_df[\"match\"].astype(float).tolist(),\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": eval_df[\"left_name\"].tolist(),\n",
    "    \"sentence2\": eval_df[\"right_name\"].tolist(),\n",
    "    \"label\": eval_df[\"match\"].astype(float).tolist(),\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": test_df[\"left_name\"].tolist(),\n",
    "    \"sentence2\": test_df[\"right_name\"].tolist(),\n",
    "    \"label\": test_df[\"match\"].astype(float).tolist(),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Initialize the SentenceTransformer model that we'll fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the SBERT model\n",
    "sbert_model = SentenceTransformer(\n",
    "    SBERT_MODEL,\n",
    "    device=str(device),\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=f\"{SBERT_MODEL}-address-matcher-{VARIANT}\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "sbert_model.gradient_checkpointing_enable()\n",
    "\n",
    "# Put the model in training mode\n",
    "sbert_model.train()\n",
    "\n",
    "# Apply quantization if not using fp16\n",
    "if not USE_FP16:\n",
    "    # Tell PyTorch to quantize the Linear layers\n",
    "    for module in sbert_model.modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            module.qconfig = tq.get_default_qat_qconfig(\"fbgemm\")\n",
    "    \n",
    "    # Prepare QAT: inserts FakeQuant and Observer modules\n",
    "    tq.prepare_qat(sbert_model, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Pre-Trained Model\n",
    "\n",
    "Before fine-tuning, let's test the base model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Testing raw (un-fine-tuned) SBERT model:\")\n",
    "examples = []\n",
    "examples.append(\n",
    "    [\"John Smith\", \"John Smith\", sbert_compare(sbert_model, \"John Smith\", \"John Smith\", use_gpu=True)]\n",
    ")\n",
    "examples.append(\n",
    "    [\"John Smith\", \"John H. Smith\", sbert_compare(sbert_model, \"John Smith\", \"John H. Smith\", use_gpu=True)]\n",
    ")\n",
    "# Russian name example\n",
    "examples.append(\n",
    "    [\"Yevgeny Prigozhin\", \"Евгений Пригожин\", sbert_compare(sbert_model, \"Yevgeny Prigozhin\", \"Евгений Пригожин\", use_gpu=True)]\n",
    ")\n",
    "# Chinese name example\n",
    "examples.append(\n",
    "    [\"Ben Lorica\", \"罗瑞卡\", sbert_compare(sbert_model, \"Ben Lorica\", \"罗瑞卡\", use_gpu=True)]\n",
    ")\n",
    "examples_df = pd.DataFrame(examples, columns=[\"sentence1\", \"sentence2\", \"similarity\"])\n",
    "display(examples_df)\n",
    "\n",
    "# Create a visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"sentence1\", y=\"similarity\", data=examples_df)\n",
    "plt.title(\"Pre-trained Model Similarity Scores\")\n",
    "plt.ylabel(\"Similarity Score\")\n",
    "plt.xlabel(\"Name Pairs\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Evaluation\n",
    "\n",
    "Evaluate the pre-trained model on our evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample a subset of the evaluation data\n",
    "sample_df = eval_df.sample(frac=0.1, random_state=RANDOM_SEED)\n",
    "if len(sample_df) < 5 and len(eval_df) >= 5:\n",
    "    sample_df = eval_df.sample(n=5, random_state=RANDOM_SEED)\n",
    "print(f\"Running initial evaluation on {device} for {len(sample_df):,} sample records\")\n",
    "\n",
    "# Compare the names and measure error\n",
    "result_df = sbert_compare_multiple_df(\n",
    "    sbert_model, sample_df[\"left_name\"], sample_df[\"right_name\"], sample_df[\"match\"], use_gpu=True\n",
    ")\n",
    "error_s = np.abs(result_df.match.astype(float) - result_df.similarity)\n",
    "score_diff_s = np.abs(error_s - sample_df.score)\n",
    "\n",
    "# Calculate statistics\n",
    "stats_df = pd.DataFrame(\n",
    "    [\n",
    "        {\"mean\": error_s.mean(), \"std\": error_s.std(), \"iqr\": iqr(error_s)},\n",
    "        {\"mean\": score_diff_s.mean(), \"std\": score_diff_s.std(), \"iqr\": iqr(score_diff_s.dropna())},\n",
    "    ],\n",
    "    index=[\"Raw SBERT\", \"Raw SBERT - Levenshtein Score\"],\n",
    ")\n",
    "print(\"\\nRaw SBERT model stats:\")\n",
    "display(stats_df)\n",
    "\n",
    "# Log metrics to W&B\n",
    "wandb.log({\n",
    "    \"raw_model/error_mean\": error_s.mean(),\n",
    "    \"raw_model/error_std\": error_s.std(),\n",
    "    \"raw_model/error_iqr\": iqr(error_s),\n",
    "})\n",
    "\n",
    "# Create dataset for binary classification evaluation\n",
    "sample_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": sample_df[\"left_name\"].tolist(),\n",
    "    \"sentence2\": sample_df[\"right_name\"].tolist(),\n",
    "    \"label\": sample_df[\"match\"].astype(float).tolist(),\n",
    "})\n",
    "\n",
    "# Set up evaluation directory\n",
    "eval_dir = f\"{SBERT_OUTPUT_FOLDER}/eval/binary_classification_evaluation_{SBERT_MODEL.replace('/', '-')}\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "evaluation_name = SBERT_MODEL.replace(\"/\", \"-\")\n",
    "\n",
    "# Use SentenceTransformers evaluator\n",
    "binary_acc_evaluator = BinaryClassificationEvaluator(\n",
    "    sentences1=sample_dataset[\"sentence1\"],\n",
    "    sentences2=sample_dataset[\"sentence2\"],\n",
    "    labels=sample_dataset[\"label\"],\n",
    "    name=evaluation_name,\n",
    ")\n",
    "binary_acc_results = binary_acc_evaluator(sbert_model)\n",
    "binary_acc_df = pd.DataFrame([binary_acc_results])\n",
    "display(binary_acc_df)\n",
    "\n",
    "# Log binary metrics to W&B\n",
    "wandb.log({\n",
    "    \"raw_model/binary_accuracy\": binary_acc_results.get(\"accuracy\", 0.0),\n",
    "    \"raw_model/binary_f1\": binary_acc_results.get(\"f1\", 0.0),\n",
    "    \"raw_model/binary_precision\": binary_acc_results.get(\"precision\", 0.0),\n",
    "    \"raw_model/binary_recall\": binary_acc_results.get(\"recall\", 0.0),\n",
    "    \"raw_model/binary_ap\": binary_acc_results.get(\"ap\", 0.0),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Setup\n",
    "\n",
    "Now we'll set up the training configuration for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up contrastive loss for training\n",
    "loss = losses.ContrastiveLoss(model=sbert_model)\n",
    "\n",
    "# Configure training arguments\n",
    "sbert_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=SBERT_OUTPUT_FOLDER,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    fp16=USE_FP16,\n",
    "    fp16_opt_level=\"O1\" if USE_FP16 else \"O0\",\n",
    "    warmup_ratio=0.1,\n",
    "    run_name=SBERT_MODEL,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=5,\n",
    "    save_steps=SAVE_EVAL_STEPS,\n",
    "    eval_steps=SAVE_EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_dir=\"./logs\",\n",
    "    weight_decay=0.02,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=OPTIMIZER,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=sbert_model,\n",
    "    args=sbert_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=binary_acc_evaluator,\n",
    "    compute_metrics=compute_sbert_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE), WandbCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Run the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model\n",
    "# Note: This will take a while - lower SAMPLE_FRACTION for faster training\n",
    "# For CPU training, use a very small sample (e.g., 0.001)\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Best model checkpoint path: {trainer.state.best_model_checkpoint}\")\n",
    "display(pd.DataFrame([trainer.evaluate()]))\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(SBERT_OUTPUT_FOLDER)\n",
    "print(f\"Saved model to {SBERT_OUTPUT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Fine-Tuned Model\n",
    "\n",
    "Let's test our fine-tuned model on the same examples to see if it improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\nTesting fine-tuned SBERT model:\\n\")\n",
    "tuned_examples = []\n",
    "tuned_examples.append(\n",
    "    [\"John Smith\", \"John Smith\", sbert_compare(sbert_model, \"John Smith\", \"John Smith\", use_gpu=True)]\n",
    ")\n",
    "tuned_examples.append(\n",
    "    [\"John Smith\", \"John H. Smith\", sbert_compare(sbert_model, \"John Smith\", \"John H. Smith\", use_gpu=True)]\n",
    ")\n",
    "# Russian name\n",
    "tuned_examples.append(\n",
    "    [\"Yevgeny Prigozhin\", \"Евгений Пригожин\", sbert_compare(sbert_model, \"Yevgeny Prigozhin\", \"Евгений Пригожин\", use_gpu=True)]\n",
    ")\n",
    "# Chinese name\n",
    "tuned_examples.append(\n",
    "    [\"Ben Lorica\", \"罗瑞卡\", sbert_compare(sbert_model, \"Ben Lorica\", \"罗瑞卡\", use_gpu=True)]\n",
    ")\n",
    "tuned_examples_df = pd.DataFrame(tuned_examples, columns=[\"sentence1\", \"sentence2\", \"similarity\"])\n",
    "display(tuned_examples_df)\n",
    "\n",
    "# Combine the pre-trained and fine-tuned results for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Name Pair\": examples_df[\"sentence1\"] + \" vs \" + examples_df[\"sentence2\"],\n",
    "    \"Pre-trained\": examples_df[\"similarity\"],\n",
    "    \"Fine-tuned\": tuned_examples_df[\"similarity\"]\n",
    "})\n",
    "\n",
    "# Plot the comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "comparison_df.plot(x=\"Name Pair\", y=[\"Pre-trained\", \"Fine-tuned\"], kind=\"bar\", figsize=(12, 6))\n",
    "plt.title(\"Pre-trained vs Fine-tuned Model Comparison\")\n",
    "plt.ylabel(\"Similarity Score\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Model Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation\n",
    "\n",
    "Let's evaluate the fine-tuned model on our test dataset and determine the optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample test data\n",
    "test_sample_size = max(int(len(test_df) * 0.1), min(len(test_df), 10))\n",
    "test_sample_df = test_df.sample(n=test_sample_size, random_state=RANDOM_SEED)\n",
    "\n",
    "# Get ground truth labels\n",
    "y_true = test_sample_df[\"match\"].astype(float).tolist()\n",
    "\n",
    "# Run inference with GPU acceleration\n",
    "print(f\"Running inference on {device} for {len(test_sample_df):,} test records\")\n",
    "y_scores = sbert_compare_multiple(\n",
    "    sbert_model, test_sample_df[\"left_name\"], test_sample_df[\"right_name\"], use_gpu=True\n",
    ")\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Calculate F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the best threshold\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
    "print(f\"Best F1 Score: {best_f1_score:.4f}\")\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "\n",
    "# Calculate additional metrics at the best threshold\n",
    "accuracy = accuracy_score(y_true, y_scores >= best_threshold)\n",
    "precision_at_best = precision_score(y_true, y_scores >= best_threshold)\n",
    "recall_at_best = recall_score(y_true, y_scores >= best_threshold)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision_at_best:.4f}\")\n",
    "print(f\"Recall: {recall_at_best:.4f}\")\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "pr_data = pd.DataFrame({\"Precision\": precision[:-1], \"Recall\": recall[:-1], \"F1 Score\": f1_scores})\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=pr_data, x=\"Recall\", y=\"Precision\", marker=\"o\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Curve (AUC = {roc_auc:.4f})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot F1 score vs threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, f1_scores)\n",
    "plt.axvline(x=best_threshold, color=\"r\", linestyle=\"--\", label=f\"Best Threshold = {best_threshold:.4f}\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(f\"F1 Score vs Threshold (Best F1 = {best_f1_score:.4f})\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Log final metrics to W&B\n",
    "wandb.log({\n",
    "    \"final/best_threshold\": best_threshold,\n",
    "    \"final/best_f1_score\": best_f1_score,\n",
    "    \"final/accuracy\": accuracy,\n",
    "    \"final/precision\": precision_at_best,\n",
    "    \"final/recall\": recall_at_best,\n",
    "    \"final/auc\": roc_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W&B Visualizations\n",
    "\n",
    "Log the PR curve to Weights & Biases and finish the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Log the precision-recall curve to W&B\n",
    "try:\n",
    "    # Convert to the format W&B expects (probabilities for binary classification)\n",
    "    y_probs_formatted = np.vstack([1 - y_scores, y_scores]).T\n",
    "    wandb.log({\"final/pr_curve\": wandb.plot.pr_curve(y_true, y_probs_formatted)})\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not log PR curve to W&B: {e}\")\n",
    "    # Log individual metrics instead\n",
    "    wandb.log({\"final/y_true\": y_true, \"final/y_scores\": y_scores.tolist()})\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing W&B Visualizations\n",
    "\n",
    "You can also access your W&B visualizations directly from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load a previous W&B run\n",
    "import wandb\n",
    "\n",
    "# You need to login first if you haven't already\n",
    "# wandb.login()\n",
    "\n",
    "# Initialize the W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Replace with your entity and project\n",
    "entity = WANDB_ENTITY\n",
    "project = WANDB_PROJECT\n",
    "\n",
    "# Get the most recent run\n",
    "try:\n",
    "    runs = api.runs(f\"{entity}/{project}\")\n",
    "    if not runs:\n",
    "        print(\"No runs found in the project.\")\n",
    "    else:\n",
    "        run = runs[0]  # Most recent run\n",
    "        print(f\"Loaded run: {run.name}\")\n",
    "        \n",
    "        # Display run summary\n",
    "        print(\"\\nRun Summary:\")\n",
    "        metrics = [k for k in run.summary._json_dict.keys() if not k.startswith('_')]\n",
    "        metrics_df = pd.DataFrame([(k, run.summary._json_dict[k]) for k in metrics], \n",
    "                                columns=[\"Metric\", \"Value\"])\n",
    "        display(metrics_df)\n",
    "        \n",
    "        # Create a link to the W&B dashboard\n",
    "        from IPython.display import display, HTML\n",
    "        display(HTML(f'<a href=\"{run.url}\" target=\"_blank\">View full results on W&B Dashboard</a>'))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading W&B run: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to fine-tune a SentenceTransformer model for multilingual entity resolution. The model can now compare names across languages and character sets, capturing semantic similarities that traditional string matching algorithms miss.\n",
    "\n",
    "Key takeaways:\n",
    "1. The pre-trained model already had some cross-lingual capabilities\n",
    "2. Fine-tuning on domain-specific data significantly improved performance\n",
    "3. GPU acceleration makes training and inference much faster\n",
    "4. Weights & Biases provides valuable visualizations and tracking for model performance\n",
    "\n",
    "To use this model in your own applications, you can load it from the saved directory and use it to compare names:\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the model\n",
    "model_path = \"data/fine-tuned-sbert-sentence-transformers-paraphrase-multilingual-MiniLM-L12-v2-original-adafactor\"\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# Compare names\n",
    "name1 = \"John Smith\"\n",
    "name2 = \"Jon Smith\"\n",
    "embedding1 = model.encode(name1)\n",
    "embedding2 = model.encode(name2)\n",
    "similarity = 1 - cosine(embedding1, embedding2)\n",
    "print(f\"Similarity: {similarity:.3f}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}